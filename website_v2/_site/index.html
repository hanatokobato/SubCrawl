<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="DESCRIPTION HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SubCrawl</title>
  <link rel="stylesheet" href="css/main.css">
<img src="img/ja.png" alt="Reid Pryzant" class="headshot">
<img src="img/en.png" alt="Reid Pryzant" class="headshot-right">
</br>
</head>
<body>
  <div class="container">
    <h1><a href="./">SubCrawl</a></h1>
    <h2 id="about">About</h2>

<p>SubCrawl aims to support the research and development of machine translation systems, information extraction, and other language processing techniques. Researchers at Stanford and Google created this corpus by crawling the internet for movie and tv subtitles, then aligining their captions. <a href="https://github.com/rpryzant/SubCrawl">Fork us on GitHub!</a></p>

<p>SubCrawlは、機械翻訳システム、情報抽出、その他の言語処理技術の研究開発をサポートすることを目的としています。 StanfordとGoogleの研究者は、映画やテレビ字幕のためにインターネットをクロールして、この字幕を作成してから、字幕を作成しました。 <a href="https://github.com/rpryzant/SubCrawl">GitHubでフォークしてください！</a></p>

<h2 id="features">Features</h2>

<ul>
  <li>A large corpus of with translations for over 3.2 million sentences.</li>
  <li>Translations of casual, and colloquial language, domains that are hard to find in JA-EN MT.</li>
  <li>Pre-processed data, including tokenized train/dev/test splits.</li>
  <li>
    <p>Code for making your own crawled datasets and tools for manipulating MT data.</p>
  </li>
  <li>320万件以上の翻訳を含む大規模な**コーパス。</li>
  <li>カジュアル、口語言語の翻訳、JA-EN MTでは見つからないドメイン。</li>
  <li>トークン化された列車/ dev / test分割を含む、前処理されたデータ。</li>
  <li>あなた自身のデータセットを作るためのコードと、MTデータを操作するツール**。</li>
</ul>

<h2 id="corpus-contents">Corpus Contents</h2>

<table>
  <thead>
    <tr>
      <th><em>Split</em></th>
      <th><em>Phrase Pairs</em></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Raw</td>
      <td>3243887</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>3239888</td>
    </tr>
    <tr>
      <td>Dev</td>
      <td>2000</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>3001</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>Corpus</th>
      <th>Vocab Size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Raw English</td>
      <td>379558 unique words</td>
    </tr>
    <tr>
      <td>Raw Japanese</td>
      <td>6113 unique characters</td>
    </tr>
  </tbody>
</table>

<h2 id="download">Download</h2>
<ul>
  <li><a href="">Code</a></li>
  <li><a href="">Raw corpus</a></li>
  <li><a href="">Official splits</a></li>
  <li><a href="">Example tokenization</a>. This pre-processed data is [BPE-tokenized] and has a vocab size of 16000.</li>
</ul>

<h2 id="contact">Contact</h2>

<ul>
  <li>Reid Pryzant: <code class="highlighter-rouge">rpryzant</code> [at] <code class="highlighter-rouge">stanford.edu</code></li>
  <li>Denny Britz: <code class="highlighter-rouge">dennybritz</code> [at] <code class="highlighter-rouge">google.com</code></li>
</ul>

<h2 id="cite">Cite</h2>

<div class="highlighter-rouge"><pre class="highlight"><code>archive or emnlp citation
</code></pre>
</div>


  </div>

<img src="img/stanford.png" alt="Reid Pryzant" class="headshot">
<img src="img/google.png" alt="Reid Pryzant" class="headshot-right">
</br>
</br>
</br>
</body>
</html>
