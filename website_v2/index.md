---
layout: default
---

## About

SubCrawl aims to support the research and development of machine translation systems, information extraction, and other language processing techniques. Researchers at Stanford and Google created this corpus by crawling the internet for movie and tv subtitles, then aligining their captions. [Fork us on GitHub!](https://github.com/rpryzant/SubCrawl)

SubCrawlは、機械翻訳システム、情報抽出、その他の言語処理技術の研究開発をサポートすることを目的としています。 StanfordとGoogleの研究者は、映画やテレビ字幕のためにインターネットをクロールして、この字幕を作成してから、字幕を作成しました。 [GitHubでフォークしてください！](https://github.com/rpryzant/SubCrawl)

## Features

* A **large** corpus of with translations for over 3.2 million sentences.
* Translations of **casual**, and **colloquial** language, domains that are hard to find in JA-EN MT. 
* **Pre-processed** data, including tokenized train/dev/test splits.
* **Code** for making your own dataset and **tools** for manipulating MT data.

* 320万件以上の翻訳を含む大規模な**コーパス。
* カジュアル、口語言語の翻訳、JA-EN MTでは見つからないドメイン。
* トークン化された列車/ dev / test分割を含む、前処理されたデータ。
* あなた自身のデータセットを作るためのコードと、MTデータを操作するツール**。

## Corpus Contents



| *Split* | *Phrase Pairs* |
|---------|----------------|
| Raw     | 3243887        |
| Train   | 3239888        |
| Dev     | 2000           |
| Test    | 3001           |


| Corpus        | Vocab Size             |
|---------------|------------------------|
| Pre-Tokenized | 16000                  |
| Raw English   | 379558 unique words    |
| Raw Japanese  | 6113 unique characters |




## Samples
* some samples 
* live demo of a trained model?

## Download
* [Code]()
* [Raw corpus]()
* [Official splits]()
* [Example tokenization]()
  * BPE with vocab size of 16000 

## Contact

* Reid Pryzant: `rpryzant` [at] `stanford.edu`
* Denny Britz: `dennybritz` [at] `google.com`


## Cite

```
archive or emnlp citation
```



